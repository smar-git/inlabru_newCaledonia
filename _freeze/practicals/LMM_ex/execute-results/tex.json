{
  "hash": "64a94e3f05654f8c191be6f348c769eb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Practical 1 - Linear (Mixed) Models\"\nexecute: \n  freeze: auto\nformat: \n  html:\n    mainfont: \"12\"\n  PrettyPDF-pdf:\n    keep-tex: false\n    number-sections: true\nembed-resources: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell}\n\n:::\n\n\n\nIn this practical we are going to fit linear (mixed) models in `inlabru`. \nWe are going to to:\n\n- Fit a [simple linear regression](#SLR)\n- Fit a [linear regression with discrete covariates and interactions](#LM_int)\n- Fit a [linear mixed model](#LMM).\n\n\n\n::: {.callout-note icon=\"false\"}\nYou can download the R-script of this practical by clicking the button below:\n\n{{< downloadthis LMM_ex.R dname=LMM_ex.R label=\"Download R script\" >}}\n:::\n\n\n\nStart by loading useful libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load libraries\"}\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n```\n:::\n\n\n\n## Simple linear regression{#SLR}\n\nWe consider a simple linear regression model with Gaussian observations \n\n$$\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n$$\n\nwhere $\\sigma^2$ is the observation error, and the mean parameter $\\mu_i$ is linked to the **linear predictor** ($\\eta_i$) through an identity function: \n$$\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i.\n$$ \n\nHere $\\mathbf{x}= (x_1,\\dots,x_N)$ is a continuous covariate and $\\beta_0, \\beta_1$ are parameters to be estimated.\n\nTo finalize the Bayesian model we assign prior distribution as $\\tau = 1/\\sigma^2\\sim\\text{Gamma}(a,b)$  and $\\beta_0,\\beta_1\\sim\\mathcal{N}(0,1/\\tau_{\\beta})$ (we will use the default prior settings in INLA for now).\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\n<div class='webex-solution'><button>Answer</button>\n\n\nThe hyperparameter vector has dimension 1, $\\pmb{\\theta} = (\\tau)$ while the latent Gaussian field $\\pmb{u} = (\\beta_0, \\beta_1)$ has dimension 2, $0$ mean, and sparse precision matrix:\n\n$$\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n$$ Note that, since $\\beta_0$ and $\\beta_1$ are fixed effects, the precision parameters $\\tau_{\\beta_0}$ and $\\tau_{\\beta_1}$ are fixed.\n\n\n</div>\n\n:::\n\n::: callout-note\nWe can write the linear predictor vector $\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)$ as\n\n$$\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n$$\n\nOur linear predictor consists then of two components: an intercept and a slope.\n:::\n\n\n### Simulate example data\n\nWe fix the model parameters $\\beta_0$, $\\beta_1$ and the hyperparameter $\\tau_y$ to a given value and simulate the data accordingly using the code below. The simulated response and covariate data are then saved in a `data.frame` object.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\" code-summary=\"Simulate Data from a LM\"}\n# set seed for reproducibility\nset.seed(1234) \n\n# Fix the model parameters\nbeta = c(2,0.5)\nsd_error = 0.1\n\n# simulate the data\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\n# create the data frame object\ndf = data.frame(y = y, x = x)  \n```\n:::\n\n\n### Fitting a linear regression model with `inlabru`\n\n------------------------------------------------------------------------\n\n**Step1: Defining model components**\n\nThe first step is to define the two model components: The intercept and the linear covariate effect.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nDefine an object called `cmp` that includes and (i) intercept `beta_0` and (ii) a covariate `x` linear effect `beta_1`.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe `cmp` object is here used to define model components. We can give them any useful names we like, in this case, `beta_0` and `beta_1`. You can remove the automatic intercept construction by adding a `-1` in the components\n\n\n</div>\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n```\n\n\n</div>\n:::\n\n:::\n\n::: callout-note\nNote that we have excluded the default Intercept term in the model by typing `-1` in the model components. However, `inlabru` has automatic intercept that can be called by typing `Intercept()` , which is one of `inlabru` special names and it is used to define a global intercept, e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n```\n:::\n\n:::\n\n\n**Step 2: Build the observation model**\n\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the `formula`, the `family` and the `data`.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nDefine a linear predictor `eta` using the component labels you have defined on the previous task.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe `eta` object defines how the components should be combined in order to define the model predictor.\n\n\n</div>\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\neta = y ~ beta_0 + beta_1\n```\n\n\n</div>\n:::\n\n:::\n\nThe likelihood for the observational model is defined using the `bru_obs()` function.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nDefine the observational model likelihood in an object called `lik` using the `bru_obs()` function.\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nThe `bru_obs` is expecting three arguments:\n\n-   The linear predictor `eta` we defined in the previous task\n-   The data likelihood (this can be specified by setting `family = \"gaussian\"`)\n-   The data set `df`\n\n\n</div>\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nlik =  bru_obs(formula = eta,\n            family = \"gaussian\",\n            data = df)\n```\n\n\n</div>\n:::\n\n:::\n\n\n**Step 3: Fit the model**\n\nWe fit the model using the `bru()` functions which takes as input the components and the observation model:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Fit LM in `inlabru`\"}\nfit.lm = bru(cmp, lik)\n```\n:::\n\n\n**Step 5: Extract results**\n\n\nThere are several ways to extract and examine the results of a fitted `inlabru` object. \n\nThe most natural place to start is to use the \n `summary()` which gives access to some basic information about model fit and estimates\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Model summaries\"}\nsummary(fit.lm)\n## inlabru version: 2.13.0.9016 \n## INLA version: 25.11.22 \n## Components: \n## Latent components:\n## beta_0: main = linear(1)\n## beta_1: main = linear(x)\n## Observation models: \n##   Family: 'gaussian'\n##     Tag: <No tag>\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ beta_0 + beta_1\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[] \n## Time used:\n##     Pre = 2.02, Running = 0.389, Post = 0.081, Total = 2.49 \n## Fixed effects:\n##         mean   sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.004 0.01      1.983    2.004      2.024 2.004   0\n## beta_1 0.497 0.01      0.477    0.497      0.518 0.497   0\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 94.85 13.41      70.43    94.22\n##                                         0.975quant  mode\n## Precision for the Gaussian observations     122.92 92.96\n## \n## Marginal log-Likelihood:  63.27 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n\nAnother way, which gives access to more complicated (and useful) output is to use the `predict()` function. \n\nBelow we  take the fitted `bru` object and use the `predict()` function to produce predictions for $\\mu$ given a new set of values for the model covariates or the original values used for the model fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n```\n:::\n\n\nThe `predict()` function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\n::: panel-tabset\n## Plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data and 95% credible intervals](LMM_ex_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\npred %>% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n```\n:::\n\n:::\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nGenerate predictions for the linear predictor $\\mu$ when the covariate has value $x_0 = 0.45$.\n\nWhat is the predicted value for $\\mu$? And what is the uncertainty?\n\n\n<div class='webex-solution'><button>Take hint</button>\n\n\nYou can create a new data frame containing the new observation $x_0$ and then use the `predict` function.\n\n\n</div>\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     x     mean        sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 0.45 2.228007 0.0119005 2.205552 2.227772 2.252161 2.227772  0.0002633579\n  mean.mc_std_err\n1    0.0003929829\n```\n\n\n:::\n\n\n</div>\n:::\n\n\nYou can see the predicted mean and sd by examining the produced `pred` object. In this case the mean is ca 2.22 with sd ca 0.01. This gives a 95% CI ca [2.20,  2.25].\n:::\n\n\n**NOTE** Now we have produced a credible interval for the expected mean $\\mu$ if we want to produce a _prediction_ interval for  a new observation $y$ we need to add the uncertainty that comes from the likelihood with precision $\\tau_y$. To do this we can again use the `predict()` function to compute  a 95% prediction interval for $y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred2 = predict(fit.lm, new_data, \n               formula = ~ {\n                 mu = beta_0 + beta_1\n                 sigma = sqrt(1/Precision_for_the_Gaussian_observations)\n                 list(q1 = qnorm(0.025, mean = mu, sd = sigma),\n                      q2 =  qnorm(0.975, mean = mu, sd = sigma))},\n               n.samples = 1000)\nround(c(pred2$q1$mean, pred2$q2$mean),2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.03 2.43\n```\n\n\n:::\n:::\n\nNotice that now the interval we obtain is larger.\n\n## Linear model with discrete variables and  interactions{#LM_int}\n\n\nWe consider now the dataset `iris`. Here data are recorded about 150 different iris flowers belonging to 3 different species (50 for each specie).\n\nYou can get more information about these data using \n\n::: {.cell}\n\n```{.r .cell-code}\n?iris\n```\n:::\n\n\nWe want to model the `Petal.length` as a function of `Sepal.length` and species.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"iris\")\n\niris %>% ggplot() + geom_point(aes(Sepal.Length, Petal.Length, color= Species)) +\n  facet_wrap(.~Species)\n```\n\n::: {.cell-output-display}\n![](LMM_ex_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n**Model 1 - Only `Species` effect**\nOur first model assumes that the Sepal length only depends on the species, which is a categoriacal variable.\n\n$$\n\\begin{aligned}\nY_i & \\sim\\mathcal{N}(\\mu_i,\\sigma_y),\\ &  i = 1,\\dots,150 \\\\\n\\mu_{i} & = \\eta_{i} = \\beta_1\\ I(\\text{obs }i\\text{belongs to species } 1  ) + \\beta_2\\ I(\\text{obs }i\\text{belongs to species } 2  ) + \\beta_3\\ I(\\text{obs }i\\text{belongs to species } 3  )\n\\end{aligned}\n$$\n\nUsing `lm()` we can fit the model as:\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 = lm(Petal.Length ~ Species, data  = iris)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Petal.Length ~ Species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.260 -0.258  0.038  0.240  1.348 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.46200    0.06086   24.02   <2e-16 ***\nSpeciesversicolor  2.79800    0.08607   32.51   <2e-16 ***\nSpeciesvirginica   4.09000    0.08607   47.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4303 on 147 degrees of freedom\nMultiple R-squared:  0.9414,\tAdjusted R-squared:  0.9406 \nF-statistic:  1180 on 2 and 147 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nNotice that `lm()` uses `setosa` as reference category, the parameter `Speciesversicolor` is then interpreted as the difference between the effect of the  reference species and effect of `versicolor` species.\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nImplement the model above using `inlabru` using the `model.matrix()` function in R.\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\n#Option 1 - Use the model.matrix() function\n\nmm = model.matrix(Petal.Length ~ Species, data  = iris)\niris1 = cbind(iris, mm)\ncmp = ~ Intercept(1) +  versicolor(Speciesversicolor, model = \"linear\") + virginica(Speciesvirginica, model  = \"linear\")\nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris1)\nfit1a = bru(cmp, lik)\n```\n\n\n</div>\n:::\n\n\n:::\n\nAnother way to fit this model is to realize that a fixed effect can be seen as an iid effect with fixed precision, one of  the `inlabru` ways to fit the model is:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~ -1 +  cov(Species, model = \"iid\", fixed = T, initial = log(0.001))\nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris)\nfit1b = bru(cmp, lik)\n```\n:::\n\nNotice that we fix the precision of the `iid` effect to the same value as the precision of the linear effects which is 0.001.\n\n\n\n\nThe fitted values can be inspected as\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1b$summary.random$cov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          ID     mean         sd 0.025quant 0.5quant 0.975quant     mode\n1     setosa 1.461995 0.06077754   1.342650 1.461995   1.581339 1.461995\n2 versicolor 4.259984 0.06077754   4.140639 4.259984   4.379329 4.259984\n3  virginica 5.551979 0.06077754   5.432634 5.551980   5.671324 5.551980\n           kld\n1 3.690567e-09\n2 3.690928e-09\n3 3.690633e-09\n```\n\n\n:::\n:::\n\n\n\n\nThe results from the `fit1b` model is not the same we get from the `lm()` fit. What is happening? It is just a matter of parametrization.\n\nIn the following we are going to recover the `lm()` results using the fitted `fit1b` object and the `predict()` function.\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nnewdata = data.frame(Species = 0)\npreds = predict(fit1b, newdata = c() , ~ data.frame(Reference = cov_latent[2],\n                                             Speciesversicolor = cov_latent[2]-cov_latent[1],\n                                             Speciesvirginica = cov_latent[3]-cov_latent[1]))\n```\n\n\n</div>\n:::\n\n\n\nTwo things to notice here:\n\n1. We have used a empty object as `newdata` in the `predict()` function.\n2. The suffix `_latent` indicates that we are interested in the latent model efffect.\n\n**Model 2 - Interaction between `Species` and `Sepal.Length`**\n\nOut second model is defined as\n$$\n\\begin{aligned}\nY_i & \\sim\\mathcal{N}(\\mu_i,\\sigma_y),\\ &  i = 1,\\dots,150 \\\\\n\\mu_{i} & = \\eta_{i} = \\beta_0 +  \\beta_1 x_i\\ I(\\text{obs }i\\text{belongs to species } 1  ) + \\beta_2x_i\\ I(\\text{obs }i\\text{belongs to species } 2  ) + \\beta_3x_i\\ I(\\text{obs }i\\text{belongs to species } 3  )\n\\end{aligned}\n$$\n\nthat is, we have a common intercept $\\beta_0$ while the linear effect of the Sepal length depends on the Species. Using `lm()` we have:\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 = lm(Petal.Length ~ Species:Sepal.Length, data = iris)\nmod2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Petal.Length ~ Species:Sepal.Length, data = iris)\n\nCoefficients:\n                   (Intercept)      Speciessetosa:Sepal.Length  \n                        0.5070                          0.1905  \nSpeciesversicolor:Sepal.Length   Speciesvirginica:Sepal.Length  \n                        0.6326                          0.7656  \n```\n\n\n:::\n:::\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nFit the same model in `inlabru` using the `model.matrix()`.\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\nmm = model.matrix(Petal.Length ~ Species:Sepal.Length, data = iris)\n\n# change the names to make the R friendly (the : makes problems when using them to identify columns)\n\ncolnames(mm) = c(\"Int\", \"Setosa_Sepal\",  \"Versicolor_Sepal\", \"Virginica_Sepal\")\n\niris2 = cbind(iris, mm)\n\ncmp = ~ Intercept(1) + int1(Setosa_Sepal, model = \"linear\") +\n  int2(Versicolor_Sepal, model = \"linear\") +\n  int3(Virginica_Sepal, model = \"linear\") \n  \nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris2)\nfit2 = bru(cmp, lik)\n```\n\n\n</div>\n:::\n\n:::\n\n\nA second option is to use a _weighted_ iid random effect with fixed precision as:\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp = ~Intercept(1)  + \n  slope(Species, Sepal.Length, model = \"iid\", fixed = T, initial = log(0.001))\n\nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris)\nfit2b = bru(cmp, lik)\n```\n:::\n\n\nNotice that this time, the parametrization of `lm()` and `inlabru` is the same, so we get the same results from the two models.\n\n\n\n::: {.callout-warning icon=\"false\"}\n## {{< bi pencil-square color=#c8793c >}} Task\n\nPlot the estimated regression lines for the three species using model `fit2b`\n\n\n\n::: {.cell webex.hide='Click here to see the solution'}\n\n<div class='webex-solution'><button>Click here to see the solution</button>\n\n```{.r .cell-code  code-fold=\"show\"}\npreds = predict(fit2b, iris, ~ Intercept + slope)\n\npp = preds %>% ggplot() + geom_line(aes(Sepal.Length, mean, group = Species, color = Species)) +\n  geom_ribbon(aes(Sepal.Length, ymin = q0.025, ymax = q0.975, \n                  group = Species, fill = Species), alpha = 0.5) +\n  geom_point(aes(Sepal.Length, Petal.Length,color = Species))\n```\n\n\n</div>\n:::\n\n:::\n\n\n## Linear Mixed Model{#LMM}\n\nFinally we are going to consider a simple linear mixed model, that is \n a simple linear regression model except with the addition that the data that comes in groups. \n \n Suppose that we want to include a random effect for each group $j$ (equivalent to adding a group random intercept). The model is then: \n\n$$\n y_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n$$\n\nHere the random group effect is given by the variable $u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)$ with $\\tau_u = 1/\\sigma^2_u$ describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, $\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)$ denotes the residuals of the model and $\\tau_\\epsilon = 1/\\sigma^2_\\epsilon$ captures how much individual observations deviate from their group mean (i.e., variability within group).\n\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the $ij$-th observation consists of zeros except for the element associated with $u_j$, which has a one.\n\n$$\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n$$\n\n::: {.callout-note icon=\"false\"}\n## Supplementary material: LMM as a LGM\n\nIn matrix form, the linear mixed model for the *j*-th group can be written as:\n\n$$ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, $$\n\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the *i*-th observation can be written as :\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n$$\n\nwhere $f_k(u_j) = u_j$ since thereâ€™s only one random effect per group (i.e., a random intercept for group $j$). The fixed effects $(\\beta_0,\\beta_1)$ are assigned Gaussian priors (e.g., $\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})$). The random effects $\\mathbf{u} = (u_1,\\ldots,u_m)^T$ follow a Gaussian density $\\mathcal{N}(0,\\mathbf{Q}_u^{-1})$ where $\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m$ is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\n-   Latent field given by\n\n    $$\n    \\begin{bmatrix} \\beta \\\\\\mathbf{u}\n    \\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n    $$\n\n-   Likelihood:\n\n    $$\n    y_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n    $$\n\n-   Hyperparameters:\n\n    -   $\\tau_u\\sim\\mathrm{Gamma}(a,b)$\n    -   $\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)$\n:::\n\n### **Simulate example data**\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Simulate data from a LMM\"}\n#|\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))\n```\n:::\n\n\nNote that `inlabru` expects an integer indexing variable to label the groups.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n```\n\n::: {.cell-output-display}\n![Data for the linear mixed model example with 5 groups](LMM_ex_files/figure-pdf/unnamed-chunk-28-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n\n\n\n### Fitting a LMM in `inlabru`\n\n------------------------------------------------------------------------\n\nThis is done in three steps:\n\n1. Define the model components\n2. Define the formula and the observation model (likelihood) using `bru_obs()`\n3. Run the model using `bru()`\n\nAfter that you can collect and inspect results.\n\n**Defining model components and observational model**\n\nIn order to specify this model we must use the `group` argument to tell `inlabru` which variable indexes the groups. The `model = \"iid\"` tells INLA that the groups are independent from one another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n```\n:::\n\n\nThe group variable is indexed by column `j` in the dataset. We have chosen to name this component `v()` to connect with the mathematical notation that we used above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n```\n:::\n\n\n**Fitting the model**\n\nThe model can be fitted exactly as in the previous examples by using the `bru` function with the components and likelihood objects.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Fit a LMM in inlabru\"}\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0.9016 \n## INLA version: 25.11.22 \n## Components: \n## Latent components:\n## beta_0: main = linear(1)\n## beta_1: main = linear(x)\n## u: main = iid(j)\n## Observation models: \n##   Family: 'gaussian'\n##     Tag: <No tag>\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[] \n## Time used:\n##     Pre = 1.25, Running = 0.261, Post = 0.0402, Total = 1.55 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name\t  Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n\n\n### Model predictions\n\nTo compute model predictions we can create a `data.frame` containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while specifying the model components.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"LMM fitted values\"}\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u)\n\n\npred %>%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) +\n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n```\n\n::: {.cell-output-display}\n![](LMM_ex_files/figure-pdf/unnamed-chunk-32-1.pdf){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n::: {.callout-tip icon=\"false\"}\n## {{< bi question-octagon color=#6dc83c >}} Question\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\n<div class='webex-solution'><button>Give me a hint</button>\n\n\nIn general, the mixed model predictor can decomposed as:\n\n$$ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} $$\n\nWhere $X$ is a $n \\times p$ design matrix and $\\beta$ the corresponding *p*-dimensional vector of fixed effects. Then $Z$ is a $n\\times q_J$ design matrix for the $q_J$ random effects and $J$ groups; $\\mathbf{v}$ is then a $q_J \\times 1$ vector of $q$ random effects for the $J$ groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\n$$ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) $$\n\n\n</div>\n\n\n\n<div class='webex-solution'><button>See Solution</button>\n\n\n-   The linear predictor is given by\n\n    $$\n    \\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n    $$\n\n-   Latent field defined by:\n\n    -   $\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})$\n\n    -   $\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})$ where the precision matrix is a block-diagonal matrix with entries $\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}$\n\n-   The hyperparameters are then:\n\n    -   $\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon$\n\nTo fit this model in `inlabru` we can simply modify the model components as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")\n```\n:::\n\n\n\n</div>\n\n:::\n",
    "supporting": [
      "LMM_ex_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}