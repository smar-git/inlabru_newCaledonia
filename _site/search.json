[
  {
    "objectID": "practicals/LMM_ex.html",
    "href": "practicals/LMM_ex.html",
    "title": "Practical 1 - Linear (Mixed) Models",
    "section": "",
    "text": "In this practical we are going to fit linear (mixed) models in inlabru. We are going to to:\nStart by loading useful libraries:\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)"
  },
  {
    "objectID": "practicals/LMM_ex.html#SLR",
    "href": "practicals/LMM_ex.html#SLR",
    "title": "Practical 1 - Linear (Mixed) Models",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nWe consider a simple linear regression model with Gaussian observations\n\\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor (\\(\\eta_i\\)) through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i.\n\\]\nHere \\(\\mathbf{x}= (x_1,\\dots,x_N)\\) is a continuous covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated.\nTo finalize the Bayesian model we assign prior distribution as \\(\\tau = 1/\\sigma^2\\sim\\text{Gamma}(a,b)\\) and \\(\\beta_0,\\beta_1\\sim\\mathcal{N}(0,1/\\tau_{\\beta})\\) (we will use the default prior settings in INLA for now).\n\n\n\n\n\n\nTip Question\n\n\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\nAnswer\n\nThe hyperparameter vector has dimension 1, \\(\\pmb{\\theta} = (\\tau)\\) while the latent Gaussian field \\(\\pmb{u} = (\\beta_0, \\beta_1)\\) has dimension 2, \\(0\\) mean, and sparse precision matrix:\n\\[\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n\\] Note that, since \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects, the precision parameters \\(\\tau_{\\beta_0}\\) and \\(\\tau_{\\beta_1}\\) are fixed.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can write the linear predictor vector \\(\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)\\) as\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n\\]\nOur linear predictor consists then of two components: an intercept and a slope.\n\n\n\nSimulate example data\nWe fix the model parameters \\(\\beta_0\\), \\(\\beta_1\\) and the hyperparameter \\(\\tau_y\\) to a given value and simulate the data accordingly using the code below. The simulated response and covariate data are then saved in a data.frame object.\n\n\nSimulate Data from a LM\n# set seed for reproducibility\nset.seed(1234) \n\n# Fix the model parameters\nbeta = c(2,0.5)\nsd_error = 0.1\n\n# simulate the data\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\n# create the data frame object\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting a linear regression model with inlabru\n\nStep1: Defining model components\nThe first step is to define the two model components: The intercept and the linear covariate effect.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine an object called cmp that includes and (i) intercept beta_0 and (ii) a covariate x linear effect beta_1.\n\n\nTake hint\n\nThe cmp object is here used to define model components. We can give them any useful names we like, in this case, beta_0 and beta_1. You can remove the automatic intercept construction by adding a -1 in the components\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we have excluded the default Intercept term in the model by typing -1 in the model components. However, inlabru has automatic intercept that can be called by typing Intercept() , which is one of inlabru special names and it is used to define a global intercept, e.g.\n\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n\n\n\nStep 2: Build the observation model\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the formula, the family and the data.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine a linear predictor eta using the component labels you have defined on the previous task.\n\n\nTake hint\n\nThe eta object defines how the components should be combined in order to define the model predictor.\n\n\n\n\nClick here to see the solution\n\n\nCode\neta = y ~ beta_0 + beta_1\n\n\n\n\n\nThe likelihood for the observational model is defined using the bru_obs() function.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine the observational model likelihood in an object called lik using the bru_obs() function.\n\n\nTake hint\n\nThe bru_obs is expecting three arguments:\n\nThe linear predictor eta we defined in the previous task\nThe data likelihood (this can be specified by setting family = \"gaussian\")\nThe data set df\n\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik =  bru_obs(formula = eta,\n            family = \"gaussian\",\n            data = df)\n\n\n\n\n\nStep 3: Fit the model\nWe fit the model using the bru() functions which takes as input the components and the observation model:\n\nfit.lm = bru(cmp, lik)\n\nStep 5: Extract results\nThere are several ways to extract and examine the results of a fitted inlabru object.\nThe most natural place to start is to use the summary() which gives access to some basic information about model fit and estimates\n\nsummary(fit.lm)\n## inlabru version: 2.13.0.9016 \n## INLA version: 25.11.22 \n## Components: \n## Latent components:\n## beta_0: main = linear(1)\n## beta_1: main = linear(x)\n## Observation models: \n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ beta_0 + beta_1\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[] \n## Time used:\n##     Pre = 1.59, Running = 0.494, Post = 0.186, Total = 2.27 \n## Fixed effects:\n##         mean   sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.004 0.01      1.983    2.004      2.024 2.004   0\n## beta_1 0.497 0.01      0.477    0.497      0.518 0.497   0\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 94.85 13.41      70.43    94.22\n##                                         0.975quant  mode\n## Precision for the Gaussian observations     122.92 92.96\n## \n## Marginal log-Likelihood:  63.27 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\nAnother way, which gives access to more complicated (and useful) output is to use the predict() function.\nBelow we take the fitted bru object and use the predict() function to produce predictions for \\(\\mu\\) given a new set of values for the model covariates or the original values used for the model fit\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\nThe predict() function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nGenerate predictions for the linear predictor \\(\\mu\\) when the covariate has value \\(x_0 = 0.45\\).\nWhat is the predicted value for \\(\\mu\\)? And what is the uncertainty?\n\n\nTake hint\n\nYou can create a new data frame containing the new observation \\(x_0\\) and then use the predict function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\npred\n\n\n     x    mean         sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 0.45 2.22803 0.01137675 2.205508 2.227992 2.250356 2.227992  0.0002644507\n  mean.mc_std_err\n1    0.0003764898\n\n\n\nYou can see the predicted mean and sd by examining the produced pred object. In this case the mean is ca 2.22 with sd ca 0.01. This gives a 95% CI ca [2.20, 2.25].\n\n\nNOTE Now we have produced a credible interval for the expected mean \\(\\mu\\) if we want to produce a prediction interval for a new observation \\(y\\) we need to add the uncertainty that comes from the likelihood with precision \\(\\tau_y\\). To do this we can again use the predict() function to compute a 95% prediction interval for \\(y\\).\n\npred2 = predict(fit.lm, new_data, \n               formula = ~ {\n                 mu = beta_0 + beta_1\n                 sigma = sqrt(1/Precision_for_the_Gaussian_observations)\n                 list(q1 = qnorm(0.025, mean = mu, sd = sigma),\n                      q2 =  qnorm(0.975, mean = mu, sd = sigma))},\n               n.samples = 1000)\nround(c(pred2$q1$mean, pred2$q2$mean),2)\n\n[1] 2.03 2.43\n\n\nNotice that now the interval we obtain is larger."
  },
  {
    "objectID": "practicals/LMM_ex.html#LM_int",
    "href": "practicals/LMM_ex.html#LM_int",
    "title": "Practical 1 - Linear (Mixed) Models",
    "section": "Linear model with discrete variables and interactions",
    "text": "Linear model with discrete variables and interactions\nWe consider now the dataset iris. Here data are recorded about 150 different iris flowers belonging to 3 different species (50 for each specie).\nYou can get more information about these data using\n\n?iris\n\nWe want to model the Petal.length as a function of Sepal.length and species.\n\ndata(\"iris\")\n\niris %&gt;% ggplot() + geom_point(aes(Sepal.Length, Petal.Length, color= Species)) +\n  facet_wrap(.~Species)\n\n\n\n\n\n\n\n\nModel 1 - Only Species effect Our first model assumes that the Sepal length only depends on the species, which is a categoriacal variable.\n\\[\n\\begin{aligned}\nY_i & \\sim\\mathcal{N}(\\mu_i,\\sigma_y),\\ &  i = 1,\\dots,150 \\\\\n\\mu_{i} & = \\eta_{i} = \\beta_1\\ I(\\text{obs }i\\text{belongs to species } 1  ) + \\beta_2\\ I(\\text{obs }i\\text{belongs to species } 2  ) + \\beta_3\\ I(\\text{obs }i\\text{belongs to species } 3  )\n\\end{aligned}\n\\]\nUsing lm() we can fit the model as:\n\nmod1 = lm(Petal.Length ~ Species, data  = iris)\nsummary(mod1)\n\n\nCall:\nlm(formula = Petal.Length ~ Species, data = iris)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.260 -0.258  0.038  0.240  1.348 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.46200    0.06086   24.02   &lt;2e-16 ***\nSpeciesversicolor  2.79800    0.08607   32.51   &lt;2e-16 ***\nSpeciesvirginica   4.09000    0.08607   47.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4303 on 147 degrees of freedom\nMultiple R-squared:  0.9414,    Adjusted R-squared:  0.9406 \nF-statistic:  1180 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that lm() uses setosa as reference category, the parameter Speciesversicolor is then interpreted as the difference between the effect of the reference species and effect of versicolor species.\n\n\n\n\n\n\nWarning Task\n\n\n\nImplement the model above using inlabru using the model.matrix() function in R.\n\n\n\nClick here to see the solution\n\n\nCode\n#Option 1 - Use the model.matrix() function\n\nmm = model.matrix(Petal.Length ~ Species, data  = iris)\niris1 = cbind(iris, mm)\ncmp = ~ Intercept(1) +  versicolor(Speciesversicolor, model = \"linear\") + virginica(Speciesvirginica, model  = \"linear\")\nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris1)\nfit1a = bru(cmp, lik)\n\n\n\n\n\nAnother way to fit this model is to realize that a fixed effect can be seen as an iid effect with fixed precision, one of the inlabru ways to fit the model is:\n\ncmp = ~ -1 +  cov(Species, model = \"iid\", fixed = T, initial = log(0.001))\nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris)\nfit1b = bru(cmp, lik)\n\nNotice that we fix the precision of the iid effect to the same value as the precision of the linear effects which is 0.001.\nThe fitted values can be inspected as\n\nfit1b$summary.random$cov\n\n          ID     mean         sd 0.025quant 0.5quant 0.975quant     mode\n1     setosa 1.461995 0.06077754   1.342650 1.461995   1.581339 1.461995\n2 versicolor 4.259984 0.06077754   4.140639 4.259984   4.379329 4.259984\n3  virginica 5.551979 0.06077754   5.432634 5.551980   5.671324 5.551980\n           kld\n1 3.690567e-09\n2 3.690928e-09\n3 3.690633e-09\n\n\nThe results from the fit1b model is not the same we get from the lm() fit. What is happening? It is just a matter of parametrization.\nIn the following we are going to recover the lm() results using the fitted fit1b object and the predict() function.\n\n\n\nClick here to see the solution\n\n\nCode\nnewdata = data.frame(Species = 0)\npreds = predict(fit1b, newdata = c() , ~ data.frame(Reference = cov_latent[2],\n                                             Speciesversicolor = cov_latent[2]-cov_latent[1],\n                                             Speciesvirginica = cov_latent[3]-cov_latent[1]))\n\n\n\nTwo things to notice here:\n\nWe have used a empty object as newdata in the predict() function.\nThe suffix _latent indicates that we are interested in the latent model efffect.\n\nModel 2 - Interaction between Species and Sepal.Length\nOut second model is defined as \\[\n\\begin{aligned}\nY_i & \\sim\\mathcal{N}(\\mu_i,\\sigma_y),\\ &  i = 1,\\dots,150 \\\\\n\\mu_{i} & = \\eta_{i} = \\beta_0 +  \\beta_1 x_i\\ I(\\text{obs }i\\text{belongs to species } 1  ) + \\beta_2x_i\\ I(\\text{obs }i\\text{belongs to species } 2  ) + \\beta_3x_i\\ I(\\text{obs }i\\text{belongs to species } 3  )\n\\end{aligned}\n\\]\nthat is, we have a common intercept \\(\\beta_0\\) while the linear effect of the Sepal length depends on the Species. Using lm() we have:\n\nmod2 = lm(Petal.Length ~ Species:Sepal.Length, data = iris)\nmod2\n\n\nCall:\nlm(formula = Petal.Length ~ Species:Sepal.Length, data = iris)\n\nCoefficients:\n                   (Intercept)      Speciessetosa:Sepal.Length  \n                        0.5070                          0.1905  \nSpeciesversicolor:Sepal.Length   Speciesvirginica:Sepal.Length  \n                        0.6326                          0.7656  \n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nFit the same model in inlabru using the model.matrix().\n\n\n\nClick here to see the solution\n\n\nCode\nmm = model.matrix(Petal.Length ~ Species:Sepal.Length, data = iris)\n\n# change the names to make the R friendly (the : makes problems when using them to identify columns)\n\ncolnames(mm) = c(\"Int\", \"Setosa_Sepal\",  \"Versicolor_Sepal\", \"Virginica_Sepal\")\n\niris2 = cbind(iris, mm)\n\ncmp = ~ Intercept(1) + int1(Setosa_Sepal, model = \"linear\") +\n  int2(Versicolor_Sepal, model = \"linear\") +\n  int3(Virginica_Sepal, model = \"linear\") \n  \nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris2)\nfit2 = bru(cmp, lik)\n\n\n\n\n\nA second option is to use a weighted iid random effect with fixed precision as:\n\ncmp = ~Intercept(1)  + \n  slope(Species, Sepal.Length, model = \"iid\", fixed = T, initial = log(0.001))\n\nlik = bru_obs(formula =Petal.Length ~ .,\n              data = iris)\nfit2b = bru(cmp, lik)\n\nNotice that this time, the parametrization of lm() and inlabru is the same, so we get the same results from the two models.\n\n\n\n\n\n\nWarning Task\n\n\n\nPlot the estimated regression lines for the three species using model fit2b\n\n\n\nClick here to see the solution\n\n\nCode\npreds = predict(fit2b, iris, ~ Intercept + slope)\n\npp = preds %&gt;% ggplot() + geom_line(aes(Sepal.Length, mean, group = Species, color = Species)) +\n  geom_ribbon(aes(Sepal.Length, ymin = q0.025, ymax = q0.975, \n                  group = Species, fill = Species), alpha = 0.5) +\n  geom_point(aes(Sepal.Length, Petal.Length,color = Species))"
  },
  {
    "objectID": "practicals/LMM_ex.html#LMM",
    "href": "practicals/LMM_ex.html#LMM",
    "title": "Practical 1 - Linear (Mixed) Models",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nFinally we are going to consider a simple linear mixed model, that is a simple linear regression model except with the addition that the data that comes in groups.\nSuppose that we want to include a random effect for each group \\(j\\) (equivalent to adding a group random intercept). The model is then:\n\\[\ny_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n\\]\nHere the random group effect is given by the variable \\(u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)\\) with \\(\\tau_u = 1/\\sigma^2_u\\) describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, \\(\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)\\) denotes the residuals of the model and \\(\\tau_\\epsilon = 1/\\sigma^2_\\epsilon\\) captures how much individual observations deviate from their group mean (i.e., variability within group).\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the \\(ij\\)-th observation consists of zeros except for the element associated with \\(u_j\\), which has a one.\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n\\]\n\n\n\n\n\n\nNoteSupplementary material: LMM as a LGM\n\n\n\nIn matrix form, the linear mixed model for the j-th group can be written as:\n\\[ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, \\]\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the i-th observation can be written as :\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n\\]\nwhere \\(f_k(u_j) = u_j\\) since there’s only one random effect per group (i.e., a random intercept for group \\(j\\)). The fixed effects \\((\\beta_0,\\beta_1)\\) are assigned Gaussian priors (e.g., \\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)). The random effects \\(\\mathbf{u} = (u_1,\\ldots,u_m)^T\\) follow a Gaussian density \\(\\mathcal{N}(0,\\mathbf{Q}_u^{-1})\\) where \\(\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m\\) is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\nLatent field given by\n\\[\n\\begin{bmatrix} \\beta \\\\\\mathbf{u}\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n\\]\nLikelihood:\n\\[\ny_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n\\]\nHyperparameters:\n\n\\(\\tau_u\\sim\\mathrm{Gamma}(a,b)\\)\n\\(\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)\\)\n\n\n\n\n\nSimulate example data\n\n#|\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))\n\nNote that inlabru expects an integer indexing variable to label the groups.\n\n\nCode\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n\n\n\n\n\nData for the linear mixed model example with 5 groups\n\n\n\n\n\n\nFitting a LMM in inlabru\n\nThis is done in three steps:\n\nDefine the model components\nDefine the formula and the observation model (likelihood) using bru_obs()\nRun the model using bru()\n\nAfter that you can collect and inspect results.\nDefining model components and observational model\nIn order to specify this model we must use the group argument to tell inlabru which variable indexes the groups. The model = \"iid\" tells INLA that the groups are independent from one another.\n\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n\nThe group variable is indexed by column j in the dataset. We have chosen to name this component v() to connect with the mathematical notation that we used above.\n\n# Construct likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nFitting the model\nThe model can be fitted exactly as in the previous examples by using the bru function with the components and likelihood objects.\n\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0.9016 \n## INLA version: 25.11.22 \n## Components: \n## Latent components:\n## beta_0: main = linear(1)\n## beta_1: main = linear(x)\n## u: main = iid(j)\n## Observation models: \n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[] \n## Time used:\n##     Pre = 1.46, Running = 0.265, Post = 0.0434, Total = 1.77 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name     Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel predictions\nTo compute model predictions we can create a data.frame containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while specifying the model components.\n\n\nLMM fitted values\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u)\n\n\npred %&gt;%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) +\n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip Question\n\n\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\nGive me a hint\n\nIn general, the mixed model predictor can decomposed as:\n\\[ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} \\]\nWhere \\(X\\) is a \\(n \\times p\\) design matrix and \\(\\beta\\) the corresponding p-dimensional vector of fixed effects. Then \\(Z\\) is a \\(n\\times q_J\\) design matrix for the \\(q_J\\) random effects and \\(J\\) groups; \\(\\mathbf{v}\\) is then a \\(q_J \\times 1\\) vector of \\(q\\) random effects for the \\(J\\) groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\\[ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) \\]\n\n\n\nSee Solution\n\n\nThe linear predictor is given by\n\\[\n\\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n\\]\nLatent field defined by:\n\n\\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)\n\\(\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})\\) where the precision matrix is a block-diagonal matrix with entries \\(\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}\\)\n\nThe hyperparameters are then:\n\n\\(\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon\\)\n\n\nTo fit this model in inlabru we can simply modify the model components as follows:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")"
  },
  {
    "objectID": "Install_software.html",
    "href": "Install_software.html",
    "title": "Software Installation",
    "section": "",
    "text": "Please use the instructions below to install and check your installation.\nIf you run into issues, you can post a question with information about what you tried and what didn’t work, on the course github discussion page and we will try reply.\nSince 29 April 2025, the latest INLA package is built for R 4.5, so if you’re able to upgrade your R installation, please do so to avoid unnecessary issues. The package will in many cases also work with older R versions, but compatibility is sometimes difficult."
  },
  {
    "objectID": "Install_software.html#installing-inla-and-inlabru",
    "href": "Install_software.html#installing-inla-and-inlabru",
    "title": "Software Installation",
    "section": "Installing INLA and inlabru",
    "text": "Installing INLA and inlabru\nDue to the work involved in building the binaries for the INLA package C software for different architectures, the INLA package is not on CRAN, but it can be installed from its own distribution repository.\n\nCheck your R version.\nInstall R-INLA, instructions can be found here\nInstall inlabru (available from CRAN)\n\n\n# Enable universe(s) by inlabru-org\noptions(repos = c(\n  inlabruorg = \"https://inlabru-org.r-universe.dev\",\n  INLA = \"https://inla.r-inla-download.org/R/testing\",\n  CRAN = \"https://cloud.r-project.org\"\n))\n\n# Install some packages\ninstall.packages(\"inlabru\")\n\n\nMake sure you have the latest R-INLA, inlabru and R versions installed.\nInstall the following libraries:\n\n\n\ninstall.packages(c(\n  \"dplyr\",\n  \"ggplot2\"\n))"
  },
  {
    "objectID": "Install_software.html#installation-check",
    "href": "Install_software.html#installation-check",
    "title": "Software Installation",
    "section": "Installation check",
    "text": "Installation check\nPlease check your installation using the basic model runs below.\nIf you run into issues, you can post a question with information about what you tried and what didn’t work, on the course github discussion page.\nYou can check that INLA is correctly installed by running\n\ndf &lt;- data.frame(y = rnorm(100) + 10)\nfit &lt;- INLA::inla(\n  y ~ 1,\n  data = df\n)\nsummary(fit)\n\nTime used:\n    Pre = 0.976, Running = 0.255, Post = 0.00848, Total = 1.24 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept) 10.104 0.104        9.9   10.104     10.307 10.104   0\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.947 0.133      0.704    0.941\n                                        0.975quant  mode\nPrecision for the Gaussian observations       1.23 0.928\n\nMarginal log-Likelihood:  -157.48 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nIf the simple inla() call fails with a crash, you may need to install different inla binaries for your hardware/software combination, with INLA::inla.binary.install().\nWhen inla() works, you can check that inlabru is installed correctly by running the same model in inlabru:\n\nfit &lt;- inlabru::bru(\n  y ~ Intercept(1, prec.linear = exp(-7)),\n  data = df\n)\nsummary(fit)\n\ninlabru version: 2.13.0.9016 \nINLA version: 25.11.22 \nComponents: \nLatent components:\nIntercept: main = linear(1)\nObservation models: \n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: y ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[Intercept], latent[] \nTime used:\n    Pre = 0.877, Running = 0.185, Post = 0.0127, Total = 1.07 \nFixed effects:\n            mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nIntercept 10.103 0.104        9.9   10.103     10.307 10.103   0\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.947 0.133      0.704    0.941\n                                        0.975quant  mode\nPrecision for the Gaussian observations       1.23 0.928\n\nMarginal log-Likelihood:  -161.95 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "inlabru Workshop",
    "section": "",
    "text": "Welcome to the course!\n\nWelcome to the inlabru workshop!\nThe aim of this workshop is to introduce you to a range of statistical modelling approaches, in particular the temporal, spatial and spatio-temporal modelling as implemented in the inlabru package.\nWorkshop materials are available in the github repository\n⚠️Important! ⚠️ Please use the instructions provided here to install and check your installation before the course start.\n\n\n\nLearning Objectives for the workshop\nAt the end of the workshop, participants will have an understanding of:\n\nthe motivation for and the challenges of analysing and modelling spatial data\nstatistical models used to analyse spatial and spatio-temporal data\nthe implementation of these models in the inlabru package\nhow to independently analyse spatial data with inlabru\n\n\n\nIntended audience\nThe workshop aims to cater for participants with a range of different backgrounds, who is interested in analysing data with modern spatial and spatio-temporal statistical modelling approaches.\n\n\nPrerequisites\nParticipants should be familiar with the R environment, and general statistical approaches for modelling such as regression, analysis of (co)variance, and generalized linear models.​\nNo knowledge of R-INLA or inlabru is required.\n\n\nSchedule\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n: {.striped .hover}\n\n\n: {.striped .hover}\n\n\n: {.striped .hover}\n\n\n: {.striped .hover}\n\n\n: {.striped .hover}"
  },
  {
    "objectID": "slides/slides_1.html#outline",
    "href": "slides/slides_1.html#outline",
    "title": "Lecture 1",
    "section": "Outline",
    "text": "Outline\n\nWhat are INLA and inlabru?\nWhy the Bayesian framework?\nWhich model are inlabru-friendly?\nWhat are Latent Gaussian Models?\nHow are they implemented in inlabru?"
  },
  {
    "objectID": "slides/slides_1.html#what-is-inla-what-is-inlabru",
    "href": "slides/slides_1.html#what-is-inla-what-is-inlabru",
    "title": "Lecture 1",
    "section": "What is INLA? What is inlabru?",
    "text": "What is INLA? What is inlabru?\nThe short answer:\n\nINLA is a fast method to do Bayesian inference with latent Gaussian models and inlabru is an R-package that implements this method with a flexible and simple interface.\n\nThe (much) longer answer:\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599."
  },
  {
    "objectID": "slides/slides_1.html#where",
    "href": "slides/slides_1.html#where",
    "title": "Lecture 1",
    "section": "Where?",
    "text": "Where?\n\n\n\n Website-tutorials\n\n\ninlabru https://inlabru-org.github.io/inlabru/\nR-INLA https://www.r-inla.org/home\n\n\n\n\n\n\n Discussion forums\n\n\ninlabru https://github.com/inlabru-org/inlabru/discussions\nR-INLA https://groups.google.com/g/r-inla-discussion-group\n\n\n\n\n\n\n Books\n\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nGómez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., Gómez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., … & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nWang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC."
  },
  {
    "objectID": "slides/slides_1.html#so-why-should-you-use-inlabru",
    "href": "slides/slides_1.html#so-why-should-you-use-inlabru",
    "title": "Lecture 1",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it?"
  },
  {
    "objectID": "slides/slides_1.html#so-why-should-you-use-inlabru-1",
    "href": "slides/slides_1.html#so-why-should-you-use-inlabru-1",
    "title": "Lecture 1",
    "section": "So… Why should you use inlabru?",
    "text": "So… Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use it `inlabru’?\n\n\nTo give proper answers to these questions, we need to start at the very beginning…"
  },
  {
    "objectID": "slides/slides_1.html#the-core",
    "href": "slides/slides_1.html#the-core",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something."
  },
  {
    "objectID": "slides/slides_1.html#the-core-1",
    "href": "slides/slides_1.html#the-core-1",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions."
  },
  {
    "objectID": "slides/slides_1.html#the-core-2",
    "href": "slides/slides_1.html#the-core-2",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions.\nWe want answers!"
  },
  {
    "objectID": "slides/slides_1.html#how-do-we-find-answers",
    "href": "slides/slides_1.html#how-do-we-find-answers",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\n\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?"
  },
  {
    "objectID": "slides/slides_1.html#how-do-we-find-answers-1",
    "href": "slides/slides_1.html#how-do-we-find-answers-1",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?\n\nThese questions are not independent."
  },
  {
    "objectID": "slides/slides_1.html#bayesian-or-frequentist",
    "href": "slides/slides_1.html#bayesian-or-frequentist",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters !"
  },
  {
    "objectID": "slides/slides_1.html#bayesian-or-frequentist-1",
    "href": "slides/slides_1.html#bayesian-or-frequentist-1",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters!\nEvery parameter is described by a probability distribution!"
  },
  {
    "objectID": "slides/slides_1.html#bayesian-or-frequentist-2",
    "href": "slides/slides_1.html#bayesian-or-frequentist-2",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no “true but unknown” parameters!\nEvery parameter is described by a probability distribution!\nEvidence from the data is used to update the belief we had before observing the data!"
  },
  {
    "objectID": "slides/slides_1.html#some-more-details-i",
    "href": "slides/slides_1.html#some-more-details-i",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define as the linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) = \\eta_i = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "slides/slides_1.html#some-more-details-i-1",
    "href": "slides/slides_1.html#some-more-details-i-1",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define as the linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) =\\eta_i =  \\color{red}{\\boxed{\\beta_0}} +  \\color{red}{\\boxed{\\beta_1 x_i}}\\)\nThis model has two components!"
  },
  {
    "objectID": "slides/slides_1.html#some-more-details-ii",
    "href": "slides/slides_1.html#some-more-details-ii",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations are independent of each other!\n\nThis means that all dependencies in the observations are accounted for by the components!"
  },
  {
    "objectID": "slides/slides_1.html#some-more-details-ii-1",
    "href": "slides/slides_1.html#some-more-details-ii-1",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations are independent of each other!\n\nThe observation model (likelihood) can be written as:\n\\[\n\\pi(\\mathbf{y}|\\eta,\\sigma^2) = \\prod_{i = 1}^n\\pi(y_i|\\eta_i,\\sigma^2)\n\\]"
  },
  {
    "objectID": "slides/slides_1.html#lets-formalize-this-a-bit",
    "href": "slides/slides_1.html#lets-formalize-this-a-bit",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of an inlabru-friendly statistical model are:\n\nThe observational model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\\\\nE(y_i|\\eta_i, \\sigma^2) & = \\eta_i\n\\end{aligned}\n\\] Note: We assume that, given the linear predictor \\(\\eta\\), the data are independent of each other! Data dependence is expressed through the components of the linear predictor."
  },
  {
    "objectID": "slides/slides_1.html#lets-formalize-this-a-bit-1",
    "href": "slides/slides_1.html#lets-formalize-this-a-bit-1",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of an inlabru-friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\beta_0 + \\beta_1x_i\n\\]"
  },
  {
    "objectID": "slides/slides_1.html#lets-formalize-this-a-bit-2",
    "href": "slides/slides_1.html#lets-formalize-this-a-bit-2",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor\n\n\\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\n\\]\nNote 1: These are the components of our model! These explain the dependence structure of the data."
  },
  {
    "objectID": "slides/slides_1.html#lets-formalize-this-a-bit-3",
    "href": "slides/slides_1.html#lets-formalize-this-a-bit-3",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\textbf{u}\\) \\[\n\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\n\\] Note: These always have a Gaussian prior and are used to explain the dependencies among observations!"
  },
  {
    "objectID": "slides/slides_1.html#lets-formalize-this-a-bit-4",
    "href": "slides/slides_1.html#lets-formalize-this-a-bit-4",
    "title": "Lecture 1",
    "section": "Let’s formalize this a bit…",
    "text": "Let’s formalize this a bit…\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\\)\nA prior for the non-Gaussian parameters \\(\\theta\\) \\[\n\\theta = \\sigma^2\n\\]"
  },
  {
    "objectID": "slides/slides_1.html#latent-gaussian-models-lgm",
    "href": "slides/slides_1.html#latent-gaussian-models-lgm",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process"
  },
  {
    "objectID": "slides/slides_1.html#latent-gaussian-models-lgm-1",
    "href": "slides/slides_1.html#latent-gaussian-models-lgm-1",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure"
  },
  {
    "objectID": "slides/slides_1.html#latent-gaussian-models-lgm-2",
    "href": "slides/slides_1.html#latent-gaussian-models-lgm-2",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters"
  },
  {
    "objectID": "slides/slides_1.html#latent-gaussian-models-lgm-3",
    "href": "slides/slides_1.html#latent-gaussian-models-lgm-3",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters\n\n\n\n\nQ: What are we interested in?"
  },
  {
    "objectID": "slides/slides_1.html#the-posterior-distribution",
    "href": "slides/slides_1.html#the-posterior-distribution",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nA\n\nPrior\n belief\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{red}{\\pi(\\mathbf{u},\\theta|\\mathbf{y})}\\propto \\color{blue}{\\pi(\\mathbf{y}|\\mathbf{u},\\theta)}\\color{green}{\\pi(\\mathbf{u}|\\theta)\\pi(\\theta)}\n\\]"
  },
  {
    "objectID": "slides/slides_1.html#the-posterior-distribution-1",
    "href": "slides/slides_1.html#the-posterior-distribution-1",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nBayesian Computation are hard!!\n Here is where\n INLA\n comes in!!!\n\n\n\nE-&gt;C\n\n\n\n\n\nA\n\nPrior\n belief\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-linear-regression",
    "href": "slides/slides_1.html#inlabru-for-linear-regression",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-linear-regression-1",
    "href": "slides/slides_1.html#inlabru-for-linear-regression-1",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-linear-regression-2",
    "href": "slides/slides_1.html#inlabru-for-linear-regression-2",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-linear-regression-3",
    "href": "slides/slides_1.html#inlabru-for-linear-regression-3",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_i, \\sigma^2}} & \\color{red}{\\boxed{\\sim \\mathcal{N}(\\eta_i,\\sigma^2)}}\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-linear-regression-4",
    "href": "slides/slides_1.html#inlabru-for-linear-regression-4",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-linear-regression-5",
    "href": "slides/slides_1.html#inlabru-for-linear-regression-5",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression"
  },
  {
    "objectID": "slides/slides_1.html#real-datasets-are-more-complicated",
    "href": "slides/slides_1.html#real-datasets-are-more-complicated",
    "title": "Lecture 1",
    "section": "Real datasets are more complicated!",
    "text": "Real datasets are more complicated!\nData can have several dependence structures: temporal, spatial,…\nUsing a Bayesian framework:\n\nBuild (hierarchical) models to account for potentially complicated dependency structures in the data.\nAttribute uncertainty to model parameters and latent variables using priors.\n\nTwo main challenges:\n\nNeed computationally efficient methods to calculate posteriors (this is where INLA helps!).\nSelect priors in a sensible way (we’ll talk about this)"
  },
  {
    "objectID": "slides/slides_1.html#the-good-news",
    "href": "slides/slides_1.html#the-good-news",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_1.html#the-good-news-1",
    "href": "slides/slides_1.html#the-good-news-1",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\n\nGaussian response? (temperature, rainfall, fish weight …)\nCount data? (people infected with a disease in each area)\nPoint pattern? (locations of trees in a forest)\nBinary data? (yes/no response, binary image)\nSurvival data? (recovery time, time to death)\n… (many more examples!!)\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_1.html#the-good-news-2",
    "href": "slides/slides_1.html#the-good-news-2",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\n\nWe assume data to be conditionally independent given the model components and some hyperparameters\nThis means that all dependencies in data are explained in Stage\n\n\n\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_1.html#the-good-news-3",
    "href": "slides/slides_1.html#the-good-news-3",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\n\nHere we can have:\n\nFixed effects for covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, )\n…\n\nThese are linked to the responses in the likelihood through linear predictors.\n\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?"
  },
  {
    "objectID": "slides/slides_1.html#the-good-news-4",
    "href": "slides/slides_1.html#the-good-news-4",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! 😃\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?\nThe likelihood and the latent model typically have hyperparameters that control their behavior.\nThey can include:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model\nVariance of unstructured effects\n…"
  },
  {
    "objectID": "slides/slides_1.html#the-second-good-news",
    "href": "slides/slides_1.html#the-second-good-news",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated your model is, the inlabru workflow is always the same 😃\n\n# Define model components\ncomps &lt;- component_1(...) +\n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1,\n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)"
  },
  {
    "objectID": "slides/slides_1.html#the-second-good-news-1",
    "href": "slides/slides_1.html#the-second-good-news-1",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated your model is, the inlabru workflow is always the same 😃\n\n# Define model components\ncomps &lt;- component_1(...) +\n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1,\n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)\n\nNOTE we will see later that this function can also be non-linear….😁"
  },
  {
    "objectID": "slides/slides_1.html#the-tokyo-rainfall-data",
    "href": "slides/slides_1.html#the-tokyo-rainfall-data",
    "title": "Lecture 1",
    "section": "The Tokyo rainfall data",
    "text": "The Tokyo rainfall data\nOne example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two years (1983-84)"
  },
  {
    "objectID": "slides/slides_1.html#the-model",
    "href": "slides/slides_1.html#the-model",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\] \\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\] \\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\n\nthe likelihood has no hyperparameters"
  },
  {
    "objectID": "slides/slides_1.html#the-model-1",
    "href": "slides/slides_1.html#the-model-1",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nprobability of rain depends on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a RW2 model (this is just a smoother). The smoothness is controlled by a hyperparameter \\(\\tau_f\\)"
  },
  {
    "objectID": "slides/slides_1.html#the-model-2",
    "href": "slides/slides_1.html#the-model-2",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field \\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\nStage 3 The hyperparameters\n\nThe structured time effect is controlled by one parameter \\(\\tau_f\\).\nWe assign a prior to \\(\\tau_f\\) to finalize the model."
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-time-series",
    "href": "slides/slides_1.html#inlabru-for-time-series",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-time-series-1",
    "href": "slides/slides_1.html#inlabru-for-time-series-1",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-time-series-2",
    "href": "slides/slides_1.html#inlabru-for-time-series-2",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_t|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Binomial}(n_t,p_t)}}\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-time-series-3",
    "href": "slides/slides_1.html#inlabru-for-time-series-3",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#example-disease-mapping",
    "href": "slides/slides_1.html#example-disease-mapping",
    "title": "Lecture 1",
    "section": "Example: disease mapping",
    "text": "Example: disease mapping\nWe observed larynx cancer mortality counts for males in 544 district of Germany from 1986 to 1990 and want to make a model.\n\n\n\n\\(y_i\\): The count at location \\(i\\).\n\\(E_i\\): An offset; expected number of cases in district \\(i\\).\n\\(c_i\\): A covariate (level of smoking consumption) at \\(i\\)\n\\(\\boldsymbol{s}_i\\): spatial location \\(i\\) ."
  },
  {
    "objectID": "slides/slides_1.html#bayesian-disease-mapping",
    "href": "slides/slides_1.html#bayesian-disease-mapping",
    "title": "Lecture 1",
    "section": "Bayesian disease mapping",
    "text": "Bayesian disease mapping\n\nStage 1: We assume the responses are Poisson distributed: \\[\ny_i \\mid \\eta_i \\sim \\text{Poisson}(E_i\\exp(\\eta_i)))\n\\]\nStage 2: \\(\\eta_i\\) is a linear function of three components: an intercept, a covariate \\(c_i\\), a spatially structured effect \\(\\omega\\) likelihood by \\[\n\\eta_i = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\]\nStage 3:\n\n\\(\\tau_{\\omega}\\): Precisions parameter for the random effects\n\n\n\nThe latent field is \\(\\boldsymbol{u} = (\\beta_0, \\beta_1, \\omega_1, \\omega_2,\\ldots, \\omega_n)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_{\\omega})\\), and must be given a prior."
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-disease-mapping",
    "href": "slides/slides_1.html#inlabru-for-disease-mapping",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1\\ c_i}} + \\color{red}{\\boxed{\\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"besag\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-disease-mapping-1",
    "href": "slides/slides_1.html#inlabru-for-disease-mapping-1",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + \\beta_1\\ c_i + \\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-disease-mapping-2",
    "href": "slides/slides_1.html#inlabru-for-disease-mapping-2",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Poisson}(E_i\\lambda_i)}}\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-disease-mapping-3",
    "href": "slides/slides_1.html#inlabru-for-disease-mapping-3",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping"
  },
  {
    "objectID": "slides/slides_1.html#bayesian-geostatistics",
    "href": "slides/slides_1.html#bayesian-geostatistics",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\nEncounter probability of Pacific Cod (Gadus macrocephalus) from a trawl survey.\n\n\n\n\n\n\n\n\\(y(s)\\) Presence or absence in location \\(s\\)"
  },
  {
    "objectID": "slides/slides_1.html#bayesian-geostatistics-1",
    "href": "slides/slides_1.html#bayesian-geostatistics-1",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_1.html#bayesian-geostatistics-2",
    "href": "slides/slides_1.html#bayesian-geostatistics-2",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\n\nA global intercept \\(\\beta_0\\)\nA smooth effect of covariate \\(x(s)\\) (depth)\nA Gaussian field \\(\\omega(s)\\) (will discuss this later..)\n\nStage 3 Hyperparameters"
  },
  {
    "objectID": "slides/slides_1.html#bayesian-geostatistics-3",
    "href": "slides/slides_1.html#bayesian-geostatistics-3",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\beta_1 x(s) + \\omega(s)\n\\]\nStage 3 Hyperparameters\n\nPrecision for the smooth function \\(f(\\cdot)\\)\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-geostatistics",
    "href": "slides/slides_1.html#inlabru-for-geostatistics",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &  = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{ f(x(s))}} + \\color{red}{\\boxed{ \\omega(s)}}\\\\\n\\end{aligned}\n\\]\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') +\n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-geostatistics-1",
    "href": "slides/slides_1.html#inlabru-for-geostatistics-1",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\color{red}{\\boxed{\\eta(s)}} &  = \\color{red}{\\boxed{\\beta_0 +  f(x(s)) +  \\omega(s)}}\\\\\n\\end{aligned}\n\\]\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') +\n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-geostatistics-2",
    "href": "slides/slides_1.html#inlabru-for-geostatistics-2",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y(s)|\\eta(s)}} & \\sim \\color{red}{\\boxed{\\text{Binom}(1, p(s))}}\\\\\n\\eta(s) &  = \\beta_0 +  f(x(s)) +  \\omega(s)\\\\\n\\end{aligned}\n\\]\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 × 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ℹ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') +\n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)"
  },
  {
    "objectID": "slides/slides_1.html#inlabru-for-geostatistics-3",
    "href": "slides/slides_1.html#inlabru-for-geostatistics-3",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics"
  },
  {
    "objectID": "slides/slides_1.html#take-home-message",
    "href": "slides/slides_1.html#take-home-message",
    "title": "Lecture 1",
    "section": "Take home message!",
    "text": "Take home message!\n\nMany of the models you have used (and some you have never used but will learn about) are just special cases of the large class of Latent Gaussian models\ninlabru provides an efficient and unified way to fit all these models!"
  }
]